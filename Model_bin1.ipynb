{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Запуск локальной сети\n",
        "\n",
        "docker run --gpus=all -p 127.0.0.1:9000:8080 us-docker.pkg.dev/colab-images/public/runtime"
      ],
      "metadata": {
        "id": "qSGfupUIlq81"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsntQmXxGyEh"
      },
      "source": [
        "# Разархивируем файлы с данными\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем файл"
      ],
      "metadata": {
        "id": "U3B-Bd7DK3es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Импорт библиотеки для работы с операционной системой\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import zipfile\n",
        "!pip install gdown\n",
        "import gdown\n",
        "def exist(filepath):\n",
        "  if not os.path.exists(file_path):\n",
        "    return True\n",
        "  return False\n",
        "def create_dirtree(dirname, name1, name2):\n",
        "    os.makedirs(dirname)\n",
        "    os.makedirs(os.path.join(dirname, name1))\n",
        "    os.makedirs(os.path.join(dirname, name2))\n",
        "def create_folders_if_not_exist():\n",
        "    data_folder = '/content/my_data'\n",
        "\n",
        "    train_data_folder = os.path.join(data_folder, 'Train')\n",
        "    val_data_folder = os.path.join(data_folder, 'Val')\n",
        "    test_data_folder = os.path.join(data_folder, 'Test')\n",
        "    model_data_folder = os.path.join(data_folder, 'Models')\n",
        "\n",
        "    if not os.path.exists(data_folder):\n",
        "        os.makedirs(data_folder)\n",
        "\n",
        "    if not os.path.exists(train_data_folder):\n",
        "        create_dirtree(train_data_folder, \"woman\", \"man\")\n",
        "\n",
        "    if not os.path.exists(val_data_folder):\n",
        "        create_dirtree(val_data_folder, \"woman\", \"man\")\n",
        "\n",
        "    if not os.path.exists(test_data_folder):\n",
        "        create_dirtree(test_data_folder, \"woman\", \"man\")\n",
        "\n",
        "    if not os.path.exists(model_data_folder):\n",
        "        os.makedirs(model_data_folder)\n",
        "\n",
        "\n",
        "create_folders_if_not_exist()\n",
        "\n",
        "# Путь к папке, куда нужно переместить архив\n",
        "destination_folder = '/content'\n",
        "\n",
        "# Имя архива\n",
        "archive_name = 'GenderData.zip'\n",
        "\n",
        "# Путь к целевой папке, куда нужно разархивировать архив\n",
        "extract_folder = '/content/my_data'\n",
        "if not os.path.exists(archive_name):\n",
        "  gdown.download('https://drive.google.com/uc?id=1EcExF7G5HOMAvZ6IZW509jivTn56uuBB')\n",
        "if not os.path.exists(\"/my_data/GenderData\"):\n",
        "      # Копируем архив в другую папку\n",
        "      shutil.copy(os.path.join(destination_folder, archive_name), extract_folder)\n",
        "      # Разархивируем архив в целевой папке\n",
        "      with zipfile.ZipFile(os.path.join(extract_folder, archive_name), 'r') as zip_ref:\n",
        "          zip_ref.extractall(extract_folder)"
      ],
      "metadata": {
        "id": "6TMxhspb0lGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c3c6ba-aad2-4fe7-bac0-6db32978a296"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EcExF7G5HOMAvZ6IZW509jivTn56uuBB\n",
            "From (redirected): https://drive.google.com/uc?id=1EcExF7G5HOMAvZ6IZW509jivTn56uuBB&confirm=t&uuid=0ed0b67b-e941-4610-a161-4dca31c88107\n",
            "To: /content/GenderData.zip\n",
            "100%|██████████| 1.40G/1.40G [00:13<00:00, 102MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_b2BLaIIJr"
      },
      "source": [
        "# Создаем train, val, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ovGm-6o8G56D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27604a4d-9717-4c02-e7af-ffead6d297d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14464\n",
            "2410\n",
            "7233\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "def clear_befor_upload(dirname):\n",
        "  os.system(f'rm -rf {dirname + \"/man/*\"}')\n",
        "  os.system(f'rm -rf {dirname + \"/woman/*\"}')\n",
        "def calculate_percentages(num1, num2, per1, per2, per3):\n",
        "    val1_m = round(num1 * per1 / 100)\n",
        "    val2_m = round(num1 * per2 / 100)\n",
        "    val3_m = round(num1 * per3 / 100)\n",
        "    # Корректируем последнее значение, чтобы сумма была равна исходному числу\n",
        "    val3_m = num1 - val1_m - val2_m\n",
        "\n",
        "    val1_w = round(num2 * per1 / 100)\n",
        "    val2_w  = round(num2 * per2 / 100)\n",
        "    val3_w  = round(num2 * per3 / 100)\n",
        "    # Корректируем последнее значение, чтобы сумма была равна исходному числу\n",
        "    val3_w  = num2 - val1_w  - val2_w\n",
        "\n",
        "    arr_train = [val1_m, val1_w]\n",
        "    arr_val = [val2_m, val2_w]\n",
        "    arr_test = [val3_m, val3_w]\n",
        "    return arr_train, arr_val, arr_test\n",
        "\n",
        "def count_files_in_directory(directory):\n",
        "    return len(list(Path(directory + \"/woman\").rglob('*.*'))) + len(list(Path(directory + \"/man\").rglob('*.*')))\n",
        "\n",
        "def move_images(num_files, source_dir, target_dir):\n",
        "    files_of_man = os.listdir(source_dir + \"/man\")\n",
        "    files_of_woman = os.listdir(source_dir + \"/woman\")\n",
        "    # Перемешиваем список файлов\n",
        "    random.shuffle(files_of_man)\n",
        "    random.shuffle(files_of_woman)\n",
        "    num_files_man = num_files[0]\n",
        "    num_files_woman = num_files[1]\n",
        "    for i in range(num_files_man):\n",
        "        shutil.move(os.path.join(source_dir + \"/man\", files_of_man[i]), os.path.join(target_dir + \"/man\", files_of_man[i]))\n",
        "    for i in range(num_files_woman):\n",
        "        shutil.move(os.path.join(source_dir + \"/woman\", files_of_woman[i]), os.path.join(target_dir+ \"/woman\", files_of_woman[i]))\n",
        "\n",
        "data_dir = \"my_data/GenderData\"\n",
        "# Каталог с данными для обучения\n",
        "train_dir = 'my_data/Train'\n",
        "# Каталог с данными для проверки\n",
        "val_dir = 'my_data/Val'\n",
        "# Каталог с данными для тестирования\n",
        "test_dir = 'my_data/Test'\n",
        "\n",
        "#Очистка папок для новых файлов\n",
        "clear_befor_upload(train_dir)\n",
        "clear_befor_upload(val_dir)\n",
        "clear_befor_upload(test_dir)\n",
        "\n",
        "# Количество элементов данных в одном классе\n",
        "num_images_man = count_files_in_directory(data_dir) - len(list(Path(data_dir + \"/woman\").rglob('*.*')))\n",
        "num_images_woman = count_files_in_directory(data_dir) - len(list(Path(data_dir + \"/man\").rglob('*.*')))\n",
        "\n",
        "num_of_train_data = 60 # Значение в процентах\n",
        "num_of_val_data = 10 # Значение в процентах\n",
        "num_of_test_data = 30 # Значение в процентах\n",
        "info_str = f\"num_of_train_data = {num_of_train_data}%\\nnum_of_val_data = {num_of_val_data}%\\nnum_of_test_data = {num_of_test_data}%\\n\"\n",
        "\n",
        "num_of_train_data, num_of_val_data, num_of_test_data = calculate_percentages(num_images_man, num_images_woman, num_of_train_data, num_of_val_data, num_of_test_data)\n",
        "\n",
        "move_images(num_of_train_data, data_dir, train_dir)\n",
        "move_images(num_of_val_data, data_dir, val_dir)\n",
        "move_images(num_of_test_data, data_dir, test_dir)\n",
        "print(count_files_in_directory(train_dir))\n",
        "print(count_files_in_directory(val_dir))\n",
        "print(count_files_in_directory(test_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce94TxQihu0h"
      },
      "source": [
        "# Подготовка данных для тестирования и обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Jk66-6LbhtyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867cd9cb-805c-4d4c-a6ce-5c29a208f8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14464 images belonging to 2 classes.\n",
            "Found 2410 images belonging to 2 classes.\n",
            "Found 7232 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "train_dir = \"/content/my_data/Train\"\n",
        "val_dir = \"/content/my_data/Val\"\n",
        "test_dir = \"/content/my_data/Test\"\n",
        "# Размеры изображения\n",
        "img_width, img_height = 224, 224\n",
        "# Размерность тензора на основе изображения для входных данных в нейронную сеть\n",
        "# backend Tensorflow, channels_last\n",
        "input_shape = (img_width,\n",
        "               img_height,\n",
        "               3)\n",
        "# Размер мини-выборки\n",
        "batch_size = 20\n",
        "info_str += f\"batch_size = {batch_size}\\n\"\n",
        "# Количество изображений для обучения\n",
        "nb_train_samples = count_files_in_directory(train_dir)\n",
        "# Количество изображений для проверки\n",
        "nb_validation_samples = count_files_in_directory(val_dir)\n",
        "# Количество изображений для тестирования\n",
        "nb_test_samples = count_files_in_directory(test_dir)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(train_dir,\n",
        "                                              target_size = (img_width, img_height),\n",
        "                                              batch_size = batch_size,\n",
        "                                              class_mode = 'binary')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(val_dir,\n",
        "                                            target_size=(img_width, img_height),\n",
        "                                            batch_size=batch_size,\n",
        "                                            class_mode='binary')\n",
        "\n",
        "test_generator = datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=(img_width, img_height),\n",
        "                                             batch_size=batch_size,\n",
        "                                             class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJjmV9evIEcu"
      },
      "source": [
        "# Создание модели\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_sam(s):\n",
        "    with open(os.path.join(\"/content/my_data/Models/\", 'training_logs.txt'), 'a', encoding='utf-8') as f:\n",
        "        print(s, file=f)\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "def save_data(model, history, info_str, scores):\n",
        "   with open(file_path, 'w', encoding='utf-8') as file:\n",
        "      file.write(f\"{info_str}\\n\")\n",
        "      # Записываем данные в файл\n",
        "      file.write(\"Эпоха\\tПотери\\t\\t\\tТочность\\t\\tВалидационные потери\\tВалидационная точность\\n\")\n",
        "      for epoch in range(len(history.history['loss'])):\n",
        "          file.write(f\"{epoch}\\t{history.history['loss'][epoch]}\\t{history.history['accuracy'][epoch]}\\t{history.history['val_loss'][epoch]}\\t{history.history['val_accuracy'][epoch]}\\n\")\n",
        "      file.write(\"\\n\")\n",
        "      model.summary(print_fn=save_model_sam)\n",
        "      file.write(f\"Колличество эпох = {epochs}\\n\")\n",
        "      file.write(f\"Доля верных ответов на тестовых данных, в процентах: {round(scores[1] * 100, 4)}\\n\")"
      ],
      "metadata": {
        "id": "quqxIhJ658VF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9yZwfYZQgDM"
      },
      "outputs": [],
      "source": [
        "import os # Импорт библиотеки для работы с операционной системой\n",
        "from keras.models import Sequential # Импорт класса для создания модели нейронной сети\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization # Импорт слоев для построения модели\n",
        "from keras.optimizers import Adam # Импорт оптимизатора\n",
        "from keras.losses import BinaryCrossentropy # Импорт функции потерь, для задач бинарной классификации\n",
        "from keras.metrics import Accuracy # Импорт метрики, которая вычисляет точность классификации\n",
        "from keras.models import load_model # Импорт функции, которая позволяет загрузить сохраненную модель Keras\n",
        "import tensorflow as tf\n",
        "\n",
        "#@markdown **Обучить модель?**\n",
        "train_model = True #@param {type:\"boolean\"}\n",
        "#@markdown **Кол-во эпох**\n",
        "epochs = 20 #@param {type:\"number\"}\n",
        "if(train_model == True):\n",
        "\n",
        "  # Создаем модель\n",
        "  model = Sequential() # Здесь мы создаем экземпляр класса Sequential, который будет нашей моделью\n",
        "  # Добавление сверточного слоя\n",
        "  model.add(Conv2D(32,\n",
        "                   (3, 3),\n",
        "                   activation = 'relu',\n",
        "                   input_shape = input_shape))\n",
        "  # Стабилизация работы\n",
        "  model.add(BatchNormalization())\n",
        "  # Добавление слоя пулинга\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # Добавление еще одного сверточного слоя\n",
        "  model.add(Conv2D(32,\n",
        "                   (5, 5),\n",
        "                   activation = 'relu',\n",
        "                   input_shape = input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  # Добавление еще одного слоя пулинга\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # Добавление еще одного сверточного слоя\n",
        "  model.add(Conv2D(64,\n",
        "                   (3, 3),\n",
        "                   activation = 'relu',\n",
        "                   input_shape = input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  # Добавление еще одного слоя пулинга\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # Добавление еще одного сверточного слоя\n",
        "  model.add(Conv2D(64,\n",
        "                   (5, 5),\n",
        "                   activation = 'relu',\n",
        "                   input_shape = input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  # Добавление еще одного слоя пулинга\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # Преобразование двумерных данных в одномерные\n",
        "  model.add(Flatten())\n",
        "  # Добавление полносвязного слоя\n",
        "  model.add(Dense(200,\n",
        "                  activation = 'relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  # Добавление выходного слоя\n",
        "  model.add(Dense(1,\n",
        "                  activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',optimizer=\"RMSprop\",metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_generator,\n",
        "                                steps_per_epoch = nb_train_samples // batch_size,\n",
        "                                epochs=epochs,\n",
        "                                validation_data = val_generator,\n",
        "                                validation_steps = nb_validation_samples // batch_size)\n",
        "\n",
        "  model.save(os.path.join(\"/content/my_data/Models/\", 'my_model.keras'))\n",
        "\n",
        "  file_path = os.path.join(\"/content/my_data/Models/\", 'training_logs.txt')\n",
        "\n",
        "\n",
        "  # Открываем файл для записи\n",
        "  with open(file_path, 'w', encoding='utf-8') as file:\n",
        "      file.write(f\"{info_str}\")\n",
        "      # Записываем данные в файл\n",
        "      file.write(\"Эпоха\\tПотери\\t\\t\\tТочность\\t\\tВалидационные потери\\tВалидационная точность\\n\")\n",
        "      for epoch in range(len(history.history['loss'])):\n",
        "          file.write(f\"{epoch}\\t{history.history['loss'][epoch]}\\t{history.history['accuracy'][epoch]}\\t{history.history['val_loss'][epoch]}\\t{history.history['val_accuracy'][epoch]}\\n\")\n",
        "else:\n",
        "  print(\"Введите имя файла: \")\n",
        "  str = input()\n",
        "  model = load_model(str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тестирование модели и запись"
      ],
      "metadata": {
        "id": "flDN-cLn02EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myprint(s):\n",
        "    with open(os.path.join(\"/content/my_data/Models/\", 'training_logs.txt'), 'a', encoding='utf-8') as f:\n",
        "        print(s, file=f)\n",
        "\n",
        "from keras.models import load_model # Импорт функции, которая позволяет загрузить сохраненную модель Keras\n",
        "\n",
        "scores = model.evaluate_generator(test_generator, nb_test_samples // batch_size)\n",
        "print(\"Колличесво эпох = \", epochs )\n",
        "print(\"Доля верных ответов на тестовых данных, в процентах:\", round(scores[1] * 100, 4))\n",
        "file_path = os.path.join(\"/content/my_data/Models/\", 'training_logs.txt')\n",
        "\n",
        "# Передайте эту функцию в model.summary()\n",
        "model.summary(print_fn=myprint)\n",
        "\n",
        "with open(file_path, 'a', encoding='utf-8') as file:\n",
        "  file.write(f\"Колличество эпох = {epochs}\\n\")\n",
        "  file.write(f\"Доля верных ответов на тестовых данных, в процентах: {round(scores[1] * 100, 4)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lF_UQz10xtu",
        "outputId": "9c1e7414-fb60-4091-c163-cf4ba861a856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-cd8fef3666f1>:7: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  scores = model.evaluate_generator(test_generator, nb_test_samples // batch_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Колличесво эпох =  20\n",
            "Доля верных ответов на тестовых данных, в процентах: 79.3629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оптимизация гиперпараметров"
      ],
      "metadata": {
        "id": "oo2-WpPXhF2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Импорт библиотеки для работы с операционной системой\n",
        "from keras.models import Sequential # Импорт класса для создания модели нейронной сети\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization # Импорт слоев для построения модели\n",
        "from keras.optimizers import Adam, Adagrad # Импорт оптимизатора\n",
        "from keras.losses import BinaryCrossentropy # Импорт функции потерь, для задач бинарной классификации\n",
        "from keras.metrics import Accuracy # Импорт метрики, которая вычисляет точность классификации\n",
        "from keras.models import load_model # Импорт функции, которая позволяет загрузить сохраненную модель Keras\n",
        "import tensorflow as tf\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    activation_choice = hp.Choice('activation', values=['relu', 'sigmoid', 'tanh', 'elu', 'selu'])\n",
        "\n",
        "   ############################################################################################################################\n",
        "    model.add(Conv2D(filters=hp.Choice('conv1_filter', min_value=32, max_value = 128, steps = 12 ),\n",
        "                     kernel_size=hp.Choice('conv1_kernel_size', values=[1, 5]),\n",
        "                     activation=activation_choice,\n",
        "                     input_shape=input_shape))\n",
        "    # Стабилизация работы\n",
        "    model.add(BatchNormalization())\n",
        "    # Добавление слоя пулинга\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # Добавление еще одного сверточного слоя\n",
        "    model.add(Conv2D(filters=hp.Choice('conv2_filter', min_value=32, max_value = 128, steps = 12 ),\n",
        "                     kernel_size=hp.Choice('conv2_kernel_size', values=[1, 5]),\n",
        "                     activation=activation_choice,\n",
        "                     input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    # Добавление еще одного слоя пулинга\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # Добавление еще одного сверточного слоя\n",
        "    model.add(Conv2D(filters=hp.Choice('conv3_filter', min_value=32, max_value = 128, steps = 12 ),\n",
        "                     kernel_size=hp.Choice('conv3_kernel_size', values=[1, 5]),\n",
        "                     activation=activation_choice,\n",
        "                     input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    # Добавление еще одного слоя пулинга\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "      # Добавление еще одного сверточного слоя\n",
        "    model.add(Conv2D(filters=hp.Choice('conv4_filter', min_value=32, max_value = 128, steps = 12 ),\n",
        "                     kernel_size=hp.Choice('conv4_kernel_size', values=[1, 5]),\n",
        "                     activation=activation_choice,\n",
        "                     input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    # Добавление еще одного слоя пулинга\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # Преобразование двумерных данных в одномерные\n",
        "    model.add(Flatten())\n",
        "    # Добавление полносвязного слоя\n",
        "    model.add(Dense(units=hp.Int('dense_1_units', min_value=32, max_value=400, step=30),\n",
        "                    activation=activation_choice))\n",
        "    model.add(Dropout(0.5))\n",
        "    # Добавление выходного слоя\n",
        "    model.add(Dense(1,\n",
        "                    activation = activation_choice))\n",
        "\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer=hp.Choice('optimizer', values=['adam','rmsprop','Adagrad']),metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "DJS9TkC9hF2O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,                 # функция создания модели\n",
        "    objective='val_accuracy',    # метрика, которую нужно оптимизировать -\n",
        "                                 # доля правильных ответов на проверочном наборе данных\n",
        "    max_trials=40,               # максимальное количество запусков обучения\n",
        "    directory='/content/my_data/Models'   # каталог, куда сохраняются обученные сети\n",
        "    )\n",
        "tuner.search_space_summary()\n",
        "tuner.search(train_generator, steps_per_epoch = nb_train_samples // batch_size,\n",
        "                                epochs=20,\n",
        "                                validation_data = val_generator,\n",
        "                                validation_steps = nb_validation_samples // batch_size)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hICzdlLPhF2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "GgsTp73GhF2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tuner.get_best_models()"
      ],
      "metadata": {
        "id": "lOnL4FpqhF2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model[0].save(os.path.join(\"/content/my_data/Models/\", 'my_gen_model.keras'))"
      ],
      "metadata": {
        "id": "hfNh7b_mhF2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сохранение данных на диск"
      ],
      "metadata": {
        "id": "sn7ROqxKhLkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjQV7GETdU8u"
      },
      "outputs": [],
      "source": [
        "import os # Импорт библиотеки для работы с операционной системой\n",
        "import zipfile # Импорт библиотеки для работы с ZIP-архивами\n",
        "import shutil # функции высокого уровня для работы с файлами и коллекциями файлов\n",
        "\n",
        "with zipfile.ZipFile('save.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk('/content/my_data/Models'):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file))\n",
        "\n",
        "    # Загрузка архива 'save.zip' на Google Drive\n",
        "    shutil.move('save.zip', '/content/drive/MyDrive/CourseWork/Models/save.zip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "9oOq8DvHKW1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QU_b2BLaIIJr",
        "Ce94TxQihu0h",
        "VJjmV9evIEcu",
        "oo2-WpPXhF2G",
        "sn7ROqxKhLkq"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}